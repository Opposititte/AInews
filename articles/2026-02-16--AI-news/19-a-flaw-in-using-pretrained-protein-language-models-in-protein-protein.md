---
title: "A flaw in using pretrained protein language models in protein–protein interaction inference models"
source: "Nature"
published: "Fri, 13 Feb 2026 00:00:00 +0000"
url: "https://www.nature.com/articles/s42256-025-01176-7"
saved_at: "2026-02-16 15:51 +0900"
cssclasses: [ai-news-note]
tags: [ai-news, ai-news/nature]
---

# A flaw in using pretrained protein language models in protein–protein interaction inference models

> [!info] Source
> - Publisher: **Nature**
> - Published: Fri, 13 Feb 2026 00:00:00 +0000
> - Link: [Source](https://www.nature.com/articles/s42256-025-01176-7)

## Overview

Abstract With the growing pervasiveness of pretrained protein language models (pLMs), pLM-based methods are increasingly being put forward for the protein–protein interaction (PPI) inference task. Here we identify and confirm that existing pretrained pLMs are a source of data leakage for the downstream PPI task. We characterize the extent of the data leakage problem by training and comparing small and efficient pLMs on a dataset that controls for data leakage (strict) with one that does not (non-strict).

## Detailed Report

### 1. What Happened

- Abstract With the growing pervasiveness of pretrained protein language models (pLMs), pLM-based methods are increasingly being put forward for the protein–protein interaction (PPI) inference task.
- Here we identify and confirm that existing pretrained pLMs are a source of data leakage for the downstream PPI task.
- We characterize the extent of the data leakage problem by training and comparing small and efficient pLMs on a dataset that controls for data leakage (strict) with one that does not (non-strict).

### 2. Why It Matters

- Abstract With the growing pervasiveness of pretrained protein language models (pLMs), pLM-based methods are increasingly being put forward for the protein–protein interaction (PPI) inference task.
- Here we identify and confirm that existing pretrained pLMs are a source of data leakage for the downstream PPI task.
- We characterize the extent of the data leakage problem by training and comparing small and efficient pLMs on a dataset that controls for data leakage (strict) with one that does not (non-strict).

### 3. Key Details

- Abstract With the growing pervasiveness of pretrained protein language models (pLMs), pLM-based methods are increasingly being put forward for the protein–protein interaction (PPI) inference task.
- Furthermore, we show that pLM-based and non-pLM-based models fail to generalize in tasks such as prediction of the human-SARS-CoV-2 PPIs or the effect of point mutations on binding affinities.
- This study demonstrates the importance of extending existing protocols for the evaluation of pLM-based models applied to paired biological datasets and identifies areas of weakness of current pLM models.
- Evaluating large language models for annotating proteins.
- Leveraging large language models for predicting microbial virulence from protein structure and sequence.

### 4. Watch Items

- Abstract With the growing pervasiveness of pretrained protein language models (pLMs), pLM-based methods are increasingly being put forward for the protein–protein interaction (PPI) inference task.
- Here we identify and confirm that existing pretrained pLMs are a source of data leakage for the downstream PPI task.
- We characterize the extent of the data leakage problem by training and comparing small and efficient pLMs on a dataset that controls for data leakage (strict) with one that does not (non-strict).

### 5. Source

> [!note] Source Link
> [Nature](https://www.nature.com/articles/s42256-025-01176-7)

## Copy/Paste (Raw Text)

Abstract With the growing pervasiveness of pretrained protein language models (pLMs), pLM-based methods are increasingly being put forward for the protein–protein interaction (PPI) inference task. Here we identify and confirm that existing pretrained pLMs are a source of data leakage for the downstream PPI task. We characterize the extent of the data leakage problem by training and comparing small and efficient pLMs on a dataset that controls for data leakage (strict) with one that does not (non-strict). Although data leakage from pretrained pLMs cause a measurable inflation of testing scores, we find that this does not necessarily extend to other, non-paired biological tasks such as protein keyword annotation. Further, we find no connection between the context lengths of pLMs and the performance of pLM-based PPI inference methods on proteins with sequence lengths that surpass it. Furthermore, we show that pLM-based and non-pLM-based models fail to generalize in tasks such as prediction of the human-SARS-CoV-2 PPIs or the effect of point mutations on binding affinities. This study demonstrates the importance of extending existing protocols for the evaluation of pLM-based models applied to paired biological datasets and identifies areas of weakness of current pLM models. Access options Access Nature and 54 other Nature Portfolio journals Get Nature+, our best-value online-access subscription 9,800 Yen / 30 days cancel any time Subscribe to this journal Receive 12 digital issues and online access to articles ￥14,900 per year only ￥1,242 per issue Buy this article ￥ 4,980 Prices may be subject to local taxes which are calculated during checkout Similar content being viewed by others Data availability All datasets generated for the analyses, and the data produced by those analyses, are available under a Creative Commons license CC BY-NC-SA 4.0 via GitHub at https://github.com/Emad-COMBINE-lab/pllm-ppi-data-leakage and via Zenodo at https://doi.org/10.5281/zenodo.17353963 (ref. 50). Code availability All codes used to conduct the analyses and create the figures are available under the GNU Affero General Public License v3 via GitHub at https://github.com/Emad-COMBINE-lab/pllm-ppi-data-leakage or via Zenodo at https://doi.org/10.5281/zenodo.17354890 (ref. 51). References Vitale, R., Bugnon, L. A., Fenoy, E. L., Milone, D. H. & Stegmayer, G. Evaluating large language models for annotating proteins. Brief. Bioinform. 25, bbae177 (2024). Quintana, F., Treangen, T. & Kavraki, L. Leveraging large language models for predicting microbial virulence from protein structure and sequence. In Proc. 14th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics 103 (Association for Computing Machinery, 2023). Zhou, K., Lei, C., Zheng, J., Huang, Y. & Zhang, Z. Pre-trained protein language model sheds new light on the prediction of Arabidopsis protein–protein interactions. Plant Methods 19, 141 (2023). Snider, J. et al. Fundamentals of protein interaction network mapping. Mol. Syst. Biol. 11, 848 (2015). Cafarelli, T. M. et al. Mapping, modeling, and characterization of protein-protein interactions on a proteomic scale. Curr. Opin. Struct. Biol. 44, 201–210 (2017). Low, T. Y. et al. Recent progress in mass spectrometry-based strategies for elucidating protein-protein interactions. Cell. Mol. Life Sci. 78, 5325–5339 (2021). Szklarczyk, D. et al. The STRING database in 2023: protein-protein association networks and functional enrichment analyses for any sequenced genome of interest. Nucleic Acids Res. 51, D638–D646 (2022). Park, Y. & Marcotte, E. M. Flaws in evaluation schemes for pair-input computational predictions. Nat. Methods 9, 1134–1136 (2012). Bernett, J., Blumenthal, D. B. & List, M. Cracking the black box of deep sequence-based protein-protein interaction prediction. Brief. Bioinform. 25, bbae076 (2024). Hamp, T. & Rost, B. More challenges for machine-learning protein interactions. Bioinformatics 31, 1521–1525 (2015). Brandes, N., Ofer, D., Peleg, Y., Rappoport, N. & Linial, M. ProteinBERT: a universal deep-learning model of protein sequence and function. Bioinformatics 38, 2102–2110 (2022). Elnaggar, A. et al. ProtTrans: toward understanding the language of life through self-supervised learning. IEEE Trans. Pattern Anal. Mach. Intell. 44, 7112–7127 (2022). Bepler, T. & Berger, B. Learning the protein language: evolution, structure, and function. Cell Systems 12, 654–669.e3 (2021). Verkuil, R. et al. Language models generalize beyond natural proteins. Preprint at bioRxiv https://doi.org/10.1101/2022.12.21.521521 (2022). Szymborski, J. & Emad, A. RAPPPID: towards generalizable protein interaction prediction with AWD-LSTM twin networks. Bioinformatics 38, 3958–3967 (2022). Chen, M. et al. Multifaceted protein–protein interaction prediction based on Siamese residual RCNN. Bioinformatics 35, i305–i314 (2019). Sledzieski, S., Singh, R., Cowen, L. & Berger, B. D-SCRIPT translates genome to phenome with sequence-based, structure-aware, genome-scale predictions of protein-protein interactions. Cell Syst. 12, 969–982.e6 (2021). Richoux, F., Servantie, C., Borès, C. & Téletchéa, S. Comparing two deep learning sequence-based models for protein–protein interaction prediction. Preprint at https://arxiv.org/abs/1901.06268 (2019). Li, Y. and Ilie, L. SPRINT: ultrafast protein-protein interaction prediction of the entire human interactome. BMC Bioinformatics 18, 485 (2017). Iandola, F. N., Shaw, A. E., Krishna, R. & Keutzer, K. W. SqueezeBERT: what can computer vision teach NLP about efficient neural networks? Preprint at https://arxiv.org/abs/2006.11316 (2020). Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: pre-training of deep bidirectional transformers for language understanding. Preprint at https://arxiv.org/abs/1810.04805 (2019). The UniProt Consortium UniProt: the Universal Protein Knowledgebase in 2023. Nucleic Acids Res. 51, D523–D531 (2023). Järvelin, K. & Kekäläinen, J. Cumulated gain-based evaluation of IR techniques. ACM Trans. Inf. Syst. 20, 422–446 (2002). Wu, X.-Z. & Zhou, Z.-H. A unified view of multi-label performance measures. Preprint at https://arxiv.org/abs/1609.00288 (2016). McHugh, M. L. Interrater reliability: the kappa statistic. Biochem. Med. 22, 276–282 (2012). Cohen, J. A coefficient of agreement for nominal scales. Educ. Psychol. Meas. 20, 37–46 (1960). Fallon, T. R. et al. Giant polyketide synthase enzymes in the biosynthesis of giant marine polyether toxins. Science 385, 671–678 (2024). Gordon, D. E. et al. A SARS-CoV-2 protein interaction map reveals targets for drug repurposing. Nature 583, 459–468 (2020). Jankauskaitė, J., Jiménez-García, B., Dapkūnas, J., Fernández-Recio, J. & Moal, I. H. SKEMPI 2.0: an updated benchmark of changes in protein–protein binding energy, kinetics and thermodynamics upon mutation. Bioinformatics 35, 462–469 (2019). Szymborski, J. & Emad, A. INTREPPPID—an orthologue-informed quintuplet network for cross-species prediction of protein–protein interaction. Brief. Bioinform. 25, bbae405 (2024). Anfinsen, C. B. Principles that govern the folding of protein chains. Science 181, 223–230 (1973). Jumper, J. et al. Highly accurate protein structure prediction with AlphaFold. Nature 596, 583–589 (2021). Bolouri, N., Szymborski, J. & Emad, A. Multi-modal protein representation learning with CLASP. Preprint at bioRxiv https://doi.org/10.1101/2025.08.10.669533 (2025). Szymborski, J. Datasets used in the INTREPPPID manuscript. Zenodo https://doi.org/10.5281/zenodo.10594150 (2024). Szymborski, J. Emad-COMBINE-lab/ppi_origami: preprint V1. Zenodo https://doi.org/10.5281/zenodo.10652234 (2024). Suzek, B. E. et al. UniRef clusters: a comprehensive and scalable alternative for improving sequence similarity searches. Bioinformatics 31, 926–932 (2015). Altschul, S. F. et al. Gapped BLAST and PSI-BLAST: a new generation of protein database search programs. Nucleic Acids Res. 25, 3389–3402 (1997). Steinegger, M. & Söding, J. MMseqs2 enables sensitive protein sequence searching for the analysis of massive data sets. Nat. Biotechnol. 35, 1026–1028 (2017). Iandola, F. N. et al. SqueezeNet: AlexNet-level accuracy with 50× fewer parameters and <0.5MB model size. Preprint at https://arxiv.org/abs/1602.07360 (2016). Hendrycks, D. & Gimpel, K. Gaussian error linear units (GELUs). Preprint at https://arxiv.org/abs/1606.08415 (2023). Srivastava, N., Hinton, G. E., Krizhevsky, A., Sutskever, I. & Salakhutdinov, R. Dropout: a simple way to prevent neural networks from overfitting. J. Mach. Learn. Res. 15, 1929–1958 (2014). Loshchilov, I. & Hutter, F. Decoupled weight decay regularization. In Proc. 7th International Conference on Learning Representations (2019). Smith, L. N. & Topin, N. Super-convergence: very fast training of neural networks using large learning rates. Preprint at https://arxiv.org/abs/1708.07120 (2018). Misra, D. Mish: a self regularized non-monotonic activation function. Preprint at https://arxiv.org/abs/1908.08681 (2019). Wan, L., Zeiler, M., Zhang, S., Le Cun, Y. & Fergus, R. Regularization of neural networks using DropConnect. In Proc. 30th International Conference on Machine Learning 1058–1066 (Proceedings of Machine Learning Research, 2013). Ridnik, T. et al. Asymmetric loss for multi-label classification. In Proc. 2021 IEEE/CVF International Conference on Computer Vision 82–91 (IEEE, 2021). Lin, T.-Y., Goyal, P., Girshick, R., He, K. & Dollár, P. Focal loss for dense object detection. In Proc. 2017 IEEE International Conference on Computer Vision 2999–3007 (IEEE, 2017). Strokach, A., Lu, T. Y. & Kim, P. M. ELASPIC2 (EL2): combining contextualized language models and graph neural networks to predict effects of mutations. J. Mol. Biol. 433, 166810 (2021). Virtanen, P. et al. SciPy 1.0: fundamental algorithms for scientific computing in Python. Nat. Methods 17, 261–272 (2020). Szymborski, J. Data for “A flaw in using pre-trained pLMs in protein-protein interaction inference models”. Zenodo https://doi.org/10.5281/zenodo.17353963 (2025). Szymborski, J. Emad-COMBINE-lab/pllm-ppi-data-leakage: v1. Zenodo https://doi.org/10.5281/zenodo.17354890 (2025). Acknowledgements This work was supported by grants from the Natural Sciences and Engineering Research Council of Canada (NSERC) (RGPIN-2019-04460; A.E.) and Canada Foundation for Innovation (CFI) JELF (project 40781; A.E.). This research was enabled in part by support provided by Calcul Québec (www.calculquebec.ca) and the Digital Research Alliance of Canada (alliancecan.ca) Author information Authors and Affiliations Contributions A.E. and J.S. contributed to project conceptualization and methodology as well as writing, editing and reviewing. A.E. contributed to supervision and funding acquisition. J.S. contributed to software, data curation, investigation and visualization. All authors read and approved the final article. Corresponding author Ethics declarations Competing interests The authors declare no competing interests. Peer review Peer review information Nature Machine Intelligence thanks Rita Casadio and Logan Hallee for their contribution to the peer review of this work. Additional information Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Rights and permissions Springer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law. About this article Cite this article Szymborski, J., Emad, A. A flaw in using pretrained protein language models in protein–protein interaction inference models. Nat Mach Intell (2026). https://doi.org/10.1038/s42256-025-01176-7 Received: Accepted: Published: Version of record: DOI: https://doi.org/10.1038/s42256-025-01176-7
